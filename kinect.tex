\chapter{Kinect}\label{sec:kinect}
%======================================================================================
Um componente criado pela Microsoft para fins recreativos (como no XBox, por exemplo), virou uma das mais conhecidas ferramentas de reconstrução 3D no cenário atual. Sua primeira versão (Kinect V1) utiliza uma técnica similar à empregada no projeto da Universidade de Stanford, com luz estruturada, porém, diferentemente dos escaners à laser, o Kinect tem um custo monetário baixo e é acessível a todo público em geral (desde entusiastas, amadores até profissionais da área). 

O Kinect V2 é composto de uma câmera RGB-D (Red, Green, Blue e Depth) e utiliza uma projeção de fótons, é mais robusto que seu antecessor, porém para ambientes fechados e para fins de escaneamento de formas humanas (como esqueleto, músculos, batimento cardíaco, por exemplo). Devido à técnica empregada para reconhecimento 3D ({\it Time Of Flight}), ele é muito sensível às texturas presentes no objeto. Ou seja, para esculturas com diferentes superfícies com diferentes refletâncias, lambertianos ou especulares, por exemplo, essas diferenças entre superfícies geram problemas nas reconstruções. Além disso, existem outros obstáculos, como a dificuldade onde o fóton emitido pelo emissor rebate em várias superfícies antes de ser detectado pelo sensor de infra-vermelho. E, em comparação ao V1, para fotogrametria de áreas externas, o custo benefício da primeira versão é mais utilizado.

O V1 é composto por 2 câmeras: uma RGB e outra de profundidade e por um projetor IR ({\it infra-red}) de padrões. E funciona da seguinte maneira: o projetor IR de padrões lança uma matriz que é conhecida pelo Kinect, a partir disso, qualquer deformação deste padrão é captada pelas câmeras, o que identifica se um objeto está no alcance dos sensores ou não. A resposta, é composta por 3 {\it outputs}: uma imagem IR,  uma RGB e a profundidade (inversa) da imagem.

\begin{figure}[!h]
	\centering
	%   \includegraphics[width=1.0\linewidth]{figs/3d-curve-sketch/system-diagram.eps}
	\includegraphics[width=0.5\linewidth]{figs/kinect.png}
	\caption{%
	Imagem de um kinect V1 aberto, constituído de uma câmera infra-vermelho (IR), uma câmera RGB e um projetor IR.
	%\cite{Cui:Theobalt:etal:PAMI2013,Pajdla:etal:ICCV2011}.
	}\label{fig:kinect}
\end{figure}

Sua principal saída da imagem do Kinect é correspondente à profundidade da cena. Em vez de providenciar uma profundidade {\it Z}, ele retorna uma profundidade inversa, {\it D}.
A profundidade da imagem é construída a partir da triangulação da imagem IR com o projetor e, consequentemente, "carregada" pela imagem IR.

\begin{figure}[!h]
	\centering
	%   \includegraphics[width=1.0\linewidth]{figs/3d-curve-sketch/system-diagram.eps}
	\includegraphics[width=1\linewidth]{figs/profundidadekinect.png}
	\caption{%
	Exemplo de como é a saída de uma imagem interpretada pelo Kinect, onde cada cor disposta na imagem, corresponde à profundidade ou distância da cena para o Kinect.
	%\cite{Cui:Theobalt:etal:PAMI2013,Pajdla:etal:ICCV2011}.
	}\label{fig:profKinect}
\end{figure}
 
Foram realizados alguns experimentos associando fotogrametria com o Kinect V1. Primeiramente, foi executado uma calibração do Kinect para este tipo de reconstrução, onde a partir de experimentos, o sistema foi modelado como \ref{eq:kinectCalibracao}.

\begin{equation}
\label{eq:kinectCalibracao}
q(z)=2.73z^{2}+0.74z 0.58
\end{equation}

Onde "z" é a profundidade em metros, e "q" a quantização.

O modelo geométrico do Kinect foi criado com um sistema multi-view considerando o RGB, IR e a profundidade.

\begin{gather} 
\label{eq:matrix}
\begin{bmatrix}
u\\
v\\
1
\end{bmatrix} 
= K
\begin{bmatrix}
s\\
t\\
1
\end{bmatrix}
\end{gather}

\begin{gather} 
\begin{bmatrix}
s\\
t\\
1
\end{bmatrix} 
= 
\underbrace{(1 + k_1r^2 + k_2r^4 + k_5r^6) 
\begin{bmatrix}
p\\
q\\
0
\end{bmatrix} }_{\text{distorção radial}} 
+
\underbrace{
\begin{bmatrix}
2k_3pq+k_4(r^2+2p^2)\\
2k_4pq+k_3(r^2+2q^2)\\
1
\end{bmatrix} }_{\text{distorção tangencial}}
\label{eq:distorcaoKinect}
\end{gather}

\begin{gather}
r^2 = p^2+q^2, 
\begin{bmatrix}
pz\\ 
qz\\ 
z
\end{bmatrix} = R(X-C)
\label{eq:relacaoKinect}
\end{gather}



Onde $k_n$ é o parâmetro de distorção, calibração $K$, rotação $R$ e centro $C$.

A profundidade é associada à geometria da câmera IR. que retorna a profundidade inversa ao longo do eixo z.

Os valores de $u$ e de $v$ são dados pela equação \ref{eq:distorcaoKinect} %(substitui-se o vetor [s, t, 1] em 2), X é o ponto coordenada 3D, c1 e c0 parâmetros do modelo.

\begin{gather} 
X_{IR} = \frac{1}{c_1 d + c_0}dis^{-1}\left ( K^{-1}_{IR}
\begin{bmatrix}
x+u_0\\ 
y+v_0\\ 
1
\end{bmatrix},k_{IR} 
\right )
\label{eq:distKinect}
\end{gather}

\begin{equation}
\label{eq:finalKinect}
u_{RGB} = K_{RGB} dis(R_{RGB}(X_{IR} - C_{RGB}),k_{RGB})
\end{equation}

Associamos o sistema de coordenadas do Kinect com a câmera IR e consequentemente, $R_{IR} = I$ (identidade) e $C_{IR} = 0$. 
O ponto 3D $X_{IR}$ é construído a partir da medição de [x,y,d] de \ref{eq:distKinect} e produz uma imagem RGB de \ref{eq:finalKinect}.

Em \ref{eq:distKinect}, $dis$ é a distorção de \ref{eq:distorcaoKinect}, $k_{IR}$ e $k_{RGB}$ são, respectivamente, distorção relacionada à IR e à RGB. 
$K{IR}$ é a calibração de IR, $K_{RGB}$ é a matriz de calibraçao. $R_{RGB}$ e $C_{RGB}$ são, a matriz de rotação e de centro da câmera RGB, respectivamente.

A calibração ocorreu usando o mesmo alvo nas câmeras IR e RGB, mesmos pontos 3D, e consequentemente, a posição relativa das câmeras.
O sistema de coordenadas global do Kinect faz a posição relativa da câmera igual a $R_{RGB}$, $C_{RGB}$.
Foi observado que existe um deslocamento entre imagem IR e a imagem da profundidade criada pelo Kinect. Para contornar este problema, uma série de experimentos foram executados, gerando \ref{tab:deslocamentoKinect}.

\begin{table}[!h]
\centering
\caption{Valores de deslocamentos e sua média}
\label{tab:deslocamentoKinect}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Imagem & 1   & 2   & 3   & 4   & Média \\ \hline
$u_0$  & 2,8 & 2,9 & 3,0 & 3,4 & 3,0   \\ \hline
$v_0$  & 3,0 & 2,7 & 2,8 & 3,1 & 2,9   \\ \hline
\end{tabular}
\end{table}

\begin{figure}[!h]
	\centering
	\subfloat[Sem o alinhamento]{\label{fig:deslocamentokinectA}\includegraphics[width=0.3\linewidth]{figs/deslocamentokinectA.png}}
	\subfloat[Com o alinhamento]{\label{fig:deslocamentokinectB}\includegraphics[width=0.3\linewidth]{figs/deslocamentokinectb.png}}
	\caption{%
	Representação visual do acerto do deslocamento. A parte em preto é a imagem IR e o contorno em branco é a imagem de profundidade do alvo.
	}\label{fig:deslocKinect}
\end{figure}

Foi observado que após a calibração, o Kinect gerava erros residuais complexos, que para compensar esse erro residual, foi criada uma correção em $z$, onde é subtraído da coordenada $Z_{IR}$ de \ref{eq:distKinect}.
Para validar essa correção, a correção-z das imagens foram construídas a partir dos resíduos das imagens ímpares e aplicadas nas pares, e o vice-versa. Depois da aplicação da correção-z , a media dos erros diminuiu aproximadamente 0,25mm.
Como parâmetro de comparação, foram dispostas 2 câmeras diferentes, no mesmo ambiente do Kinect.


\begin{figure}[!h]
	\centering
	%   \includegraphics[width=1.0\linewidth]{figs/3d-curve-sketch/system-diagram.eps}
	\includegraphics[width=1\linewidth]{figs/ambientekinect.png}
	\caption{%
	Posição e orientação do Kinect (com as câmeras IR e RGB) e o par estéreo SLR ({\it Left}, {\it Right}) em conjunto com pontos de calibração 3D reconstruídos em alvos de calibração planar.
	%\cite{Cui:Theobalt:etal:PAMI2013,Pajdla:etal:ICCV2011}.
	}\label{fig:ambienteKinect}
\end{figure}

\begin{table}[htbp]
\caption{Resultados dos testes executados no ambiente descrito anteriormente}
\label{tab:resultadosKinect}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\multirow{2}{1.5cm}{Método}& \multicolumn{3}{p{5cm}|}{Erro geométrico $e$ [mm]} \bigstrut \\
\cline{2-4} & \multicolumn{1}{c|}{$\mu$($e$)} & \multicolumn{1}{c|}{$\sigma$($e$)} & \multicolumn{1}{c|}{max($e$)} \bigstrut \\ \hline
SLR Stereo & 1,57 & 1,15 & 7,38 \bigstrut \\ \hline
Kinect & 2,39 & 1,67 & 8,64 \bigstrut \\ \hline
SR-4000 & 27,62 & 18,20 & 133,85 \bigstrut \\ 
\hline
\end{tabular}
\end{center}
\end{table}



Kinect com SfM:

A figura a seguir compara a superfície 3D de nuvem de pontos com uma com kinect. O resultado é tão bom quanto ao mais acurado multi-view stereo.

O kinect tem a capacidade e, com o procedimento de calibração, é possível combiná-lo com SfM e multi-view stereo, o que abre uma nova área de aplicação.

Quanto a qualidade da reconstrução de multi-view, o kinect ficou melhor que o SR-4000 e perto do 3.5M SLR Stereo \ref{tab:resultadosKinect}.

%COLOCAR IMAGEM COMPARATIVA AQUI%
Existem alguns {\it softwares} para utilizar o Kinect como uma ferramenta de reconstrução 3D e a maioria deles são bem acessíveis. Uma delas é o {\it Skanect} que tem a versão gratuita onde é possĩvel fazer escaneamentos básicos e a versão paga, que possibilita uma configuração maior, como por exemplo, a delimitação do objeto que será reconstruído, exportar o arquivo em diferentes formatos ({\it .PLY}; {.\it OBJ}: formato para exportação para programas que melhoram o modelo gerado (blender ou sculptris, por exemplo). E escolher o numero de faces a ser exportado; {\it STL}: próprio para a impressora 3D (software cura);{\it VRML}: salva também as cores do modelo.)

Entretanto, uma desvantagem que diminui a aplicabilidade do Kinect é que ele foi projetado para funcionar bem em espaços fechados, com detecção de formas humanas e movimentações. Ou seja, numa aplicação {\it in situ} ele já não funcionaria muito bem, pois além de não conseguir projetar os detalhes em alta definição de uma escultura, ele necessita de uma fonte de energia externa, o que dificulta a acessibilidade do mesmo e como gera uma reconstrução em tempo real (não tem uma forma de salvar em {\it cache} ou internamente), ele precisa estar ligado a um computador para fazer o escaneamento.