\doublespacing
\chapter{Reconstrução 3D passiva}\label{cap:pontosdeinteresse}
%======================================================================================
\doublespacing
\section{Introdução}

A maioria dos métodos de reconstrução 3D robusta de estrutura por movimento, ou \emph{Structure
from Motion} (SfM), tem como base a utilização de pontos de interesse
(\emph{features}), que são pontos ou áreas em comum entre as imagens a serem
reconstruídas. Para encontrar estes pontos, diferentes algoritmos são empregados. 
A maioria dos métodos baseados em SfM estima os modelos de câmeras e
realiza reconstruções esparsas de nuvens de pontos, e é seguida de abordagens denominadas
\emph{Multi-View Stereo} (MVS) para obtenção de informação 3D mais completa.  O
pipeline SfM+MVS funciona com os seguintes passos, ilustrados na Figura~\ref{fig:sfmpipeline}:

\begin{itemize}[leftmargin=2.5cm]
\item{Aquisição de imagens;}
\item{Estimação dos parâmetros de câmera para cada imagem;}
\item{Reconstrução da geometria 3D de uma cena em diversos níveis de detalhe.}
\end{itemize}

\begin{figure}[!h]
	\centering
	\caption{Procedimento da maioria dos sistemas SfM}
	\includegraphics[width=0.98\linewidth]{figs/pipelinesfm.png}
	\note{% 
	Além dessas etapas, geralmente a reconstrução é densificada usando-se MVS.
	}
	\source{
	 \citeauthor{theia-manual},
   	 \citeyear{theia-manual}.
   }
   \label{fig:sfmpipeline}
\end{figure}

Neste trabalho, abordaremos o uso de dois softwares baseados em SfM+MVS: o
MVE~\cite{mve} e o VisualSfM~\cite{wu2011visualsfm}, que julgamos serem os mais
consolidados em pesquisa prática de ponta atualmente. Ambos são grandes pacotes
que controlam inúmeros outros softwares e bibliotecas para os passos
supracitados, que posteriormente serão comentados. O que difere um do outro é a
partir da etapa de reconstrução esparsa e densa, onde o MVE emprega um algoritmo
de agregação de mapas de profundidade, enquanto o VisualSfM fornece ouros
algoritmos de densificação, sendo mais fortemente calcado no resultado esparso do SfM.

O passo de aquisição de imagens será discutido em detalhes no Capítulo de
experimentos~\ref{sec:experiments}. Seguimos adiante com os algoritmos para
processamento. Note-se que será usado o termo curto ``ponto'' para denotar
``ponto de interesse''.

\section{SIFT -- \emph{Scale Invariant Feature Transform}}

Primeiramente, calcula-se o SIFT (algoritmo de detecção de pontos de interesse,
invariante à escala e à transformações, como rotação, translação e iluminação da
imagem~\ref{fig:Sift}) em cada imagem~\cite{Lowe:IJCV2004}.

\begin{figure}[!h]
	\centering
	\caption{Imagens de correspondências do SIFT}
	\includegraphics[width=0.3\linewidth]{figs/indioSIFT.jpg}
	\includegraphics[width=0.3\linewidth]{figs/SIFTsapo.jpg}
	\includegraphics[width=0.3\linewidth]{figs/SIFTsapo2.jpg}
	\legend{Cada linha pode ser interpretada como uma correspondência encontrada entre as duas imagens.}
	\source{O autor,
			2017
			}
	\label{fig:Sift}
\end{figure}

\newpage

\noindent O algoritmo pode ser dividido em cinco etapas:

\begin{itemize}[leftmargin=2.5cm]
	\item{Deteccão de extremos no espaço-escala -- \emph{Scale-space Extrema Detection};}
	\item{Localização de pontos -- \emph{Keypoint Localization};}
	\item{Atribuição de orientação -- \emph{Orientation Assignment};}
	\item{Descritor de pontos -- \emph{Keypoint Descriptor};}
	\item{Casamento de pontos -- \emph{Keypoint Matching};}
\end{itemize}


\subsection{Detecção de extremos de espaço-escala}\label{DiffGaussian}

% From the image above, it is obvious that we can't use the same window to detect keypoints with different scale. It is OK with small corner. But to detect larger corners we need larger windows. For this, scale-space filtering is used. In it, Laplacian of Gaussian is found for the image with various σ values. LoG acts as a blob detector which detects blobs in various sizes due to change in σ. In short, σ acts as a scaling parameter. For eg, in the above image, gaussian kernel with low σ gives high value for small corner while guassian kernel with high σ fits well for larger corner. So, we can find the local maxima across the scale and space which gives us a list of (x,y,σ) values which means there is a potential keypoint at (x,y) at σ scale.

Em casos com cantos pequenos, uma detecção clássica de cantos funciona
bem~\cite{Harris:Stephens:Edge:Corner}. Porém, raramente
utilizaremos a mesma janela para detectar pontos em imagens com
diferentes escalas, pois utilizamos também imagens de maior resolução e, consequentemente, cantos
com mais área de suporte. Para isso, precisamos de janelas grandes também. 

Para resolver este problema, o filtro de escala-espaço é usado: o laplaciano
com uma gaussiana (\emph{Laplacian of Gaussian} --  LoG). O LoG atua como um detector de
particulas em diferentes tamanhos $\sigma$. (Onde $\sigma$ é o parâmetro de
escala). Por exemplo, o núcleo gaussiano com $\sigma$ baixo, tem como resposta
um alto valor para um canto pequeno. Enquanto um núcleo gaussiano com alto
$\sigma$, se encaixa bem para um canto maior. Com esta lógica, podemos encontrar
um máximo local através da escala e o espaço, o que nos fornece uma lista de
$(x,y \sigma)$, o que significa que existe um ponto-chave em potencial, com o
par $(x,y)$ na escala $\sigma$.  Por exemplo, um pixel é comparado com seus 8 vizinhos, assim
como comparado com os 9 pixels na próxima escala e os 9 pixels na escala
anterior. Se esse pixel é um local extremo, ele é um ponto-chave em potencial
nessa escala. Sua posição é posteriormente interpolada e refinada antes de
produzir os pontos finais.


% \begin{equation}
% LoG(x,y) = - \frac{1}{\pi \sigma ^4 }\left [ 1 - \frac{x^2+y^2}{2 \sigma ^2} \right ]e^{- \frac{x^2+y^2}{2 \sigma ^2}} 
% \end{equation}

Como o LoG é um pouco custoso computacionalmente, o SIFT utiliza um
algoritmo aproximado do LoG, o DoG (Diferença de Gaussianos -- \emph{Difference
of Gaussians}). O DoG é a diferença de um filtro de borramento gaussiano de uma imagem com
valores diferentes de escala $\sigma$.
Uma aplicação prática do filtro DoG é a sequência de imagens na
Figura~\ref{fig:lenadog}.

\begin{figure} [!h]
	\centering
	\caption{Exemplo de filtro gaussiano}
	\includegraphics[width=0.20\linewidth]{figs/lena.jpg}(a)
	\includegraphics[width=0.20\linewidth]{figs/lenaSigma1.png}(b)
	\includegraphics[width=0.20\linewidth]{figs/lenaSigma2.png}(c)
 	\includegraphics[width=0.20\linewidth]{figs/lenaDoG.png} (d)
	\legend{%
	 (a) - filtro gaussiano aplicado na imagem; (b) - resultado do filtro; (c) - outra filtragem gaussiana; (d) - filtro DoG.
	}
	\note{%
	O filtro gaussiano é aplicado na imagem, com $\sigma = 1$. Após isso, outra filtragem gaussiana com $\sigma = 2$ é aplicada. Subtrai-se as duas imagens e é obtido o filtro DoG.
	}
	\source{O autor,
			2017}
	\label{fig:lenadog}
\end{figure}

%\begin{figure}
%  \centering
%  \includegraphics[width=0.35\linewidth]{figs/sift_local_extrema.jpg}
%  \caption{%
%  Exemplo de funcionamento de deteccão de espaço-escala extrema
%  }\label{fig:extrema}
%\end{figure}

Da literatura~\cite{culjak2012brief}, em linguagem matemática, o DoG pode ser expresso a partir da seguinte sequência
de expressões: 
\begin{align}
\label{eq:gaussiano}
	&g_\sigma(x) = \frac{1}{2 \pi \sigma ^2} e^{-\frac{1}{2} \frac{x^T x}{\sigma
  ^2}}\\
\label{eq:gaussianScaleSpace}
	&I_\sigma = g_\sigma * I,\ \ \ \sigma \geq 0\\
\label{eq:DoG}
	&DoG_\sigma(o,s) = I_\sigma(o,s+1) - I_\sigma(o,s) \approx \triangledown^2
  g_\sigma(x),
\end{align}
Onde a equação~\ref{eq:gaussiano} é o núcleo gaussiano e $\triangledown^2$ é o operador Laplaciano.

\subsection{Atribuição de orientação}

Uma orientação é atribuída a cada ponto-chave para obter a invariância à rotação da imagem. 
Ao redor da localização do ponto, uma vizinhança é obtida, dependente da escala,
com tamanho e orientação definido a partir da magnitude do gradiente,
Equação~\ref{eq:magnitudeSIFT} e da direção do gradiente,
Equação~\ref{eq:direcaoSIFT}, 
\begin{equation}
	m(x,y) = \sqrt{(L(x+1,y)-L(x-1,y))^2 + (L(x,y+1)-L(x,y-1)^2)},
	\label{eq:magnitudeSIFT}
\end{equation}
\begin{equation}
	\theta = tan^{-1} \left(
  \frac{(L(x,y+1)-L(x,y-1)}{(L(x+1,y)-L(x-1,y)}\right),
	\label{eq:direcaoSIFT}
\end{equation}
onde $L(x,y) = I_\sigma(x,y)$ para o valor de $\sigma$ mais próximo possível da
escala do ponto de interesse em questão.  Em seguida, é montado um histograma de 36
orientações (\emph{bins}) cobrindo 360, que para cada orientação agrega o número
de píxels na vizinhança que têm orientação do gradiente compatível. O histograma é ponderado
pela magnitude do gradiente e por uma Gaussiana circular onde $\sigma$ vale
$1.5$ em relação à escala do ponto-chave \ref{fig:histogramaOrientado}.

\begin{figure} [!h]
	\centering
	\caption{%
	Exemplo do resultado obtido do histograma de orientações do gradiente.
	}
	\includegraphics[width=0.45\linewidth]{figs/histogramaOrientado.png}
	\label{fig:histogramaOrientado}
	\source{
   	 \citeauthor{jindal2010sift},
   	 \citeyear{jindal2010sift}.
   }
\end{figure}

O ponto mais alto do histograma é obtido e qualquer pico acima de 80\% desse
valor é considerado no cálculo da orientação final atribuída ao ponto. 

\subsection{Descritor de pontos}

Com os pontos criados e atribuídos com o histograma 
supracitado de 36 entradas, cria-se agora o descritor de pontos-chaves.
Uma vizinhança 16x16px ao redor do ponto-chave é escolhida e esta mesma vizinhaça
é dividida em 16 sub-blocos 4x4px. Para cada bloco, um novo histograma orientado com 8
\emph{bin} é criado. Logo, temos 128 valores válidos. Esses valores são
representados em forma de vetor para expressar o descritor de pontos-chaves
\ref{fig:descritorkeypoint}.  

\begin{figure} [!h]
	\centering
	\caption{Exemplo de um descritor de pontos SIFT}
	\includegraphics[width=0.45\linewidth]{figs/descritorKeypoint.png}
	\legend{%
	Um descritor usando uma matriz 2x2 e uma região 8x8
	}
	\source{
   	 \citeauthor{jindal2010sift},
   	 \citeyear{jindal2010sift}.
   }
	\label{fig:descritorkeypoint}
\end{figure}

\subsection{Casamento de pontos}

Em sistemas SfM, pontos-chaves são casados entre duas imagens; as imagens são
todas comparadas, duas a duas, exceto em sistemas aproximados de muito larga escala.
Tais pontos são casados a partir da identificação da
vizinhança mais próxima no espaço de atributos 128-Dimensional. Mas, em alguns
casos, a segunda combinação mais próxima pode ser parecida com a primeira. Isso
se dá por padrões repetidos e ruídos presentes nas imagens, caso em que os
sitemas SfM preferem descartar numa primeira instância, e focar apenas em
correspondências confiáveis (sem confusão em potencial).  Nesse caso, a razão da distância
mais próxima para a segunda distância mais próxima é utilizada. Se essa razão
for maior que 0.8, essa combinação é descartada. Apesar de ser uma regra
simples, é extremamente eficaz para obter as correspondências confiáveis
necessárias para a estimação das câmeras em sistemas SfM.

\section{Triangulação -- \emph{Full pair-wise image matching}}

Com os pontos de interesse (\emph{features}) extraídos, e alguma forma de
obter os modelos das câmeras, podemos, a princípio, fazer a
triangulação entre os pontos das imagens.  
A triangulação nada mais é que uma estimativa de um ponto em 3 dimensões, dado
pelo menos duas câmeras conhecidas, onde, cada câmera com a projeção do
\emph{feature} correspondente àquele ponto 3D \ref{fig:triangulacao}.

\begin{figure} [!h]
	\centering
	\caption{Exemplo de triangulação}
	\includegraphics[width=0.45\linewidth]{figs/triangulacao.png}
	\legend{%
	A imagem acima é um exemplo de triangulação utilizando um ponto qualquer, $X_j$. Onde cada câmera $C_1, C_2, C_3$ possui um \emph{feature} correspondente a cada uma delas, respectivamente, $X_{1j}, X_{2j}, X_{3j}$.
	}\source{Fergus, 
			2013.}
	\label{fig:triangulacao}
\end{figure}

Infelizmente, não é tão simples assim. Existem muitos fatores que contribuem
para aumentar a dificuldade da triangulação: ruídos, erro na estimação das
câmeras, posição inadequada das câmeras, o
feixe das projeções podem não se encontrar no mesmo ponto 3D, ou não se tem
informação alguma das câmeras, dentre outros. Entretanto, existem diversos algoritmos
para resolução de cada um dos problemas enfretados. 

Com a extração dos \emph{features} das imagens selecionadas, as próximas etapas
da reconstrução para os sistemas MVE e VisualSfM se diferem, e a partir de
agora, faremos abordagens individuais para cada um deles.

\input{mve}
\input{visualsfm}
